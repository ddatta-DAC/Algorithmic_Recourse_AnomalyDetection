{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f17d39b-5535-4d5d-8950-a93625e1b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from AD_model.AD_processor import AD_processor\n",
    "from joblib import Parallel,delayed\n",
    "import multiprocessing as MP\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    file_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    CONFIG_FILE = os.path.join( file_path ,'config.yaml')\n",
    "except: \n",
    "    CONFIG_FILE = 'config.yaml'\n",
    "\n",
    "\n",
    "\n",
    "class anomDataFetcher:\n",
    "    # -------------------------\n",
    "    # Read in anomaly data \n",
    "    # -------------------------\n",
    "    def __init__(self,DIR):\n",
    "        self.DIR = DIR\n",
    "        self.id_col = 'PanjivaRecordID'\n",
    "        try:\n",
    "            file_path = os.path.dirname(os.path.realpath(__file__))\n",
    "            CONFIG_FILE = os.path.join( file_path ,'config.yaml')\n",
    "        except: \n",
    "            CONFIG_FILE = 'config.yaml'\n",
    "        with open(CONFIG_FILE) as fh:\n",
    "            self.config = yaml.safe_load(fh)\n",
    "        Path(self.config['save_dir']).mkdir(exist_ok=True, parents=True)\n",
    "        Path(os.path.join(self.config['save_dir'], self.DIR)).mkdir(exist_ok=True, parents=True)  \n",
    "        if not self.__checkExists__():\n",
    "            self.load_data()\n",
    "        \n",
    "    def __checkExists__(self):\n",
    "        for file in self.config['read_files']:\n",
    "            fpath = os.path.join(self.config['save_dir'], self.DIR, file)\n",
    "            if not os.path.exists(fpath):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def load_data(self):  \n",
    "        ad_proc_obj = AD_processor(self.DIR)\n",
    "        normal_trainData = pd.read_csv(os.path.join(self.config['data_loc'], DIR, 'train_data.csv'), index_col=None)\n",
    "        score_dict_n = ad_proc_obj.score_samples_batch(normal_trainData.copy(deep=True))   \n",
    "        K = self.config['normalData_scorePerc_threshold']\n",
    "        scoreCutOff_normalData_dict = {}\n",
    "        # calculate nth-percentile\n",
    "        for emb, scores in score_dict_n.items():\n",
    "            scoreCutOff_normalData_dict[emb] = np.percentile(scores, K)\n",
    "        \n",
    "        for file in self.config['read_files']:\n",
    "            # read file\n",
    "            fpath = os.path.join(self.config['data_loc'], DIR, self.config['anomalySubDir'], file)\n",
    "            df = pd.read_csv(fpath, index_col=None)\n",
    "            id_list = df[self.id_col].values.tolist()\n",
    "            _scores_dict = ad_proc_obj.score_samples_batch(df.copy(deep=True))\n",
    "            tmp_df = pd.DataFrame(columns = [self.id_col,'score'])\n",
    "            num_keys = len(list(_scores_dict.keys()))\n",
    "            for emb, scores in _scores_dict.items():\n",
    "                tmp_data = [[i,j] for i,j in zip(id_list,scores)]\n",
    "                tmp_data_df = pd.DataFrame(tmp_data, columns = [self.id_col,'score'])\n",
    "                tmp_data_df = tmp_data_df.loc[tmp_data_df['score'] < scoreCutOff_normalData_dict [emb]]\n",
    "                tmp_df = tmp_df.append(tmp_data_df,ignore_index=True)\n",
    "            # select id s which are present in all cases\n",
    "            tmp_1 = tmp_df.groupby([self.id_col]).size().reset_index(name='count')\n",
    "            valid_ids = tmp_1.loc[tmp_1['count']==num_keys][self.id_col].values.tolist()\n",
    "            tmp_df = tmp_df.loc[tmp_df[self.id_col].isin(valid_ids)]\n",
    "            tmp_df = tmp_df.groupby(['PanjivaRecordID']).mean().reset_index(drop=False)\n",
    "            tmp_df = tmp_df.sort_values(by=['score'],ascending=True)\n",
    "            # Pick the most \"anomalous\"\n",
    "            valid_ids = tmp_df.head( self.config['record_count'])[self.id_col].values.tolist()\n",
    "            df = df.loc[df[self.id_col].isin(valid_ids)]\n",
    "            # Save file\n",
    "           \n",
    "            df.to_csv(\n",
    "                os.path.join( self.config['save_dir'], self.DIR, file),\n",
    "                index = False\n",
    "            ) \n",
    "        return                    \n",
    "     \n",
    "    # ======================================\n",
    "    # Auxillary function\n",
    "    # ======================================\n",
    "    def func_getPertIdx(self, df_row_true, df_row_target):\n",
    "        cols = list(df_row_true.to_dict().keys())\n",
    "        cols.remove(self.id_col)\n",
    "        res =  []\n",
    "        for i,c in enumerate(cols):\n",
    "            if df_row_true[c] != df_row_target[c]:\n",
    "                res.append(i)\n",
    "        res = (df_row_true[self.id_col] ,res)\n",
    "        return res\n",
    "    \n",
    "    # ===============================\n",
    "    # Main function to get data \n",
    "    # ===============================\n",
    "    \n",
    "    def fetch_data(self):\n",
    "        result_dict = {}\n",
    "        \n",
    "        for file in self.config['read_files']:\n",
    "            fpath = os.path.join(self.config['save_dir'], self.DIR, file)\n",
    "            df = pd.read_csv(fpath,index_col=None)\n",
    "            p_fname = file.replace('.csv','') + '_pertIdx.pkl'\n",
    "            pert_file_path = os.path.join(self.config['save_dir'], self.DIR, p_fname) \n",
    "            # Check if the perturbations are stored\n",
    "            if not os.path.exists( pert_file_path ):\n",
    "                # find the perturbed idx\n",
    "                replace_str = str(df.iloc[0][self.id_col])[-6:] # e.g 001001,001002\n",
    "                data_loc =  os.path.join(self.config['data_loc'],self.DIR, 'test_data.csv')\n",
    "                test_df = pd.read_csv(data_loc, index_col=None)\n",
    "\n",
    "                df_1 = df.copy(deep=True)\n",
    "                df_1[self.id_col] = df_1[self.id_col].apply(lambda x: int(str(x).replace(replace_str,'')))\n",
    "\n",
    "                test_df = test_df.loc[test_df[self.id_col].isin(list(df_1[self.id_col]))]\n",
    "\n",
    "                _perturbations = Parallel(n_jobs=MP.cpu_count())(\n",
    "                    delayed(self.func_getPertIdx)(test_df.iloc[i], df_1.iloc[i]) for i in tqdm(range(len(test_df))))\n",
    "\n",
    "                perturbations = OrderedDict({})\n",
    "                for _ in _perturbations:\n",
    "                    _id = int(str(_[0]) + replace_str)\n",
    "                    perturbations[_[0]] = _[1:]\n",
    "                with open(pert_file_path,'wb') as fh:\n",
    "                    pickle.dump(perturbations, fh, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "            else:\n",
    "                with open(pert_file_path,'rb') as fh:\n",
    "                    perturbations = pickle.load(fh)\n",
    "                    \n",
    "            result_dict[file] = {\n",
    "                'data': df, \n",
    "                'perturbations_labels': perturbations\n",
    "            }\n",
    "        return result_dict\n",
    "\n",
    "# DIR = 'us_import3'\n",
    "# obj = anomDataFetcher(DIR)\n",
    "# result_dict = obj.fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a67b8917-61a3-49db-88a6-830a57541eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PanjivaRecordID</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>108189304001001</td>\n",
       "      <td>0.139693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10135</th>\n",
       "      <td>107936991001001</td>\n",
       "      <td>0.142492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14423</th>\n",
       "      <td>108104276001001</td>\n",
       "      <td>0.146614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17236</th>\n",
       "      <td>108201113001001</td>\n",
       "      <td>0.148251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>108109059001001</td>\n",
       "      <td>0.149682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21358</th>\n",
       "      <td>108339039001001</td>\n",
       "      <td>0.336233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27225</th>\n",
       "      <td>108563770001001</td>\n",
       "      <td>0.336233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>107680422001001</td>\n",
       "      <td>0.336238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>107594919001001</td>\n",
       "      <td>0.336243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24345</th>\n",
       "      <td>108449130001001</td>\n",
       "      <td>0.336244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PanjivaRecordID     score\n",
       "16933  108189304001001  0.139693\n",
       "10135  107936991001001  0.142492\n",
       "14423  108104276001001  0.146614\n",
       "17236  108201113001001  0.148251\n",
       "14586  108109059001001  0.149682\n",
       "...                ...       ...\n",
       "21358  108339039001001  0.336233\n",
       "27225  108563770001001  0.336233\n",
       "3373   107680422001001  0.336238\n",
       "1237   107594919001001  0.336243\n",
       "24345  108449130001001  0.336244\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1919415-c91d-46bc-b9ad-ebbb7e87fb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
