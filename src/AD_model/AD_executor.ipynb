{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07b0242a-438b-476d-94eb-1e483b2e8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "from pandarallel import pandarallel\n",
    "from joblib import Parallel,delayed\n",
    "pandarallel.initialize()\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import wrap_non_picklable_objects\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import combinations\n",
    "from glob import glob\n",
    "from utils import util\n",
    "from typing import *\n",
    "\n",
    "# ======================================== #\n",
    "\n",
    "from model_AD_1 import AD_model_container, MEAD\n",
    "\n",
    "# ======================================== #\n",
    "\n",
    "DEVICE =  torch.device(\"cpu\")\n",
    "CONFIG_FILE = './config.yaml'\n",
    "ad_model_data_dir = None\n",
    "MEAD_emb_list = []\n",
    "CONFIG = None\n",
    "domain_dims = None\n",
    "train_epochs = 100\n",
    "error_tol = 0.001\n",
    "learning_rate = 0.001\n",
    "model_save_dir = None\n",
    "num_entities = 0\n",
    "id_col = 'PanjivaRecordID'\n",
    "threshold_dict = {}\n",
    "model_dict = {}\n",
    "\n",
    "def get_domain_dims(data_loc):\n",
    "    # with open(os.path.join(data_loc,'domain_dims.pkl'),'rb') as fh:\n",
    "    #     domain_dims = pickle.load(fh)\n",
    "    \n",
    "    domain_dims = pd.read_csv(os.apth.join(data_loc, 'data_dimensions.csv'),index=None)\n",
    "    return domain_dims\n",
    "\n",
    "'''\n",
    "Set up globals\n",
    "'''\n",
    "def setup_config(subDIR):\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global DIR, ad_model_data_dir, MEAD_emb_list, batch_size, error_tol, train_epochs, learning_rate, model_save_dir\n",
    "    global domain_dims, num_entities\n",
    "    \n",
    "    DIR = subDIR\n",
    "    with open(CONFIG_FILE, 'r') as fh:\n",
    "        CONFIG = yaml.safe_load(fh)\n",
    "        \n",
    "    ad_model_data_dir = os.path.join(\n",
    "        CONFIG['DATA_LOC'], \n",
    "        DIR,\n",
    "        CONFIG['AD_model_data_subdir']\n",
    "    )\n",
    "    data_loc = CONFIG['DATA_LOC']\n",
    "    domain_dims = get_domain_dims(os.path.join(data_loc, subDIR))\n",
    "    MEAD_emb_list = CONFIG['emb_dims']\n",
    "    MEAD_emb_list = [int(_) for _ in MEAD_emb_list]\n",
    "    train_epochs = CONFIG['train_epochs']\n",
    "    learning_rate = CONFIG['learning_rat']\n",
    "    batch_size = CONFIG['batch_size']\n",
    "    error_tol = CONFIG['batch_size']\n",
    "    model_save_dir = os.path.join(CONFIG['model_save_dir'], DIR )\n",
    "    num_entities = np.sum(domain_dims['dimension'].values)\n",
    "    return \n",
    "\n",
    "\n",
    "'''\n",
    "Procedure to train the models\n",
    "'''\n",
    "def train_AD_models():\n",
    "    global DIR, ad_model_data_dir, MEAD_emb_list, train_epochs, model_save_dir, DEVICE\n",
    "    \n",
    "    def aux(emb_dim, num_entities):\n",
    "        global ad_model_data_dir, train_epochs, learning_rate, batch_size, error_tol, model_save_dir, DEVICE  \n",
    "        train_x_pos = os.path.join(ad_model_data_dir, 'train_x_pos.npy')\n",
    "        train_x_neg = os.path.join(ad_model_data_dir, 'train_x_neg.npy')\n",
    "        ad_obj = AD_model_container(\n",
    "            emb_dim,\n",
    "            num_entities,\n",
    "            device = DEVICE,\n",
    "            lr = learning_rate\n",
    "        ) \n",
    "        \n",
    "        ad_obj.train(\n",
    "            train_x_pos, \n",
    "            train_x_neg, \n",
    "            batch_size = batch_size, \n",
    "            epochs = train_epochs,\n",
    "            error_tol = error_tol\n",
    "        )\n",
    "        \n",
    "        ad_obj.save_model( \n",
    "            model_save_dir\n",
    "        )\n",
    "        return \n",
    "        \n",
    "    Parallel(n_jobs=3)(delayed(aux)(emb_dim, num_entities,) for emb_dim in MEAD_emb_list)\n",
    "    \n",
    "    return \n",
    "    \n",
    "    \n",
    "\n",
    "'''\n",
    "Stores the pth percentile values for the likelihood scores of training samples.\n",
    "'''\n",
    "def calculate_thresholds(percentile_cutoff:List = [2,5,10]):\n",
    "    global model_save_dir\n",
    "    global MEAD_emb_list\n",
    "    global num_entities\n",
    "    global ad_model_data_dir\n",
    "    model_dict = {}\n",
    "    for emb_dim in MEAD_emb_list:\n",
    "        model_file_path = sorted(glob(os.path.join(model_save_dir, '**{}_**.pth'.format(emb_dim))))[0]\n",
    "        ad_obj = AD_model_container(\n",
    "            emb_dim,\n",
    "            num_entities\n",
    "        ) \n",
    "        ad_obj.load_model(model_file_path)\n",
    "        model_dict[emb_dim] = ad_obj\n",
    "    \n",
    "    dict_embDim_thresholdValue = defaultdict() \n",
    "    # Load the training data set \n",
    "    train_x_pos = os.path.join(ad_model_data_dir, 'train_x_pos.npy')\n",
    "    for emb_dim in MEAD_emb_list:\n",
    "        ad_obj =  model_dict[emb_dim]\n",
    "        scores = ad_obj.score_samples(train_x_pos)\n",
    "        dict_embDim_thresholdValue[emb_dim] = {}\n",
    "        # Calculate the n-th percentile values\n",
    "        for p in percentile_cutoff:\n",
    "            dict_embDim_thresholdValue[emb_dim][p] = np.percentile(np.array(scores).reshape(-1), p)\n",
    "    '''\n",
    "    Save the values\n",
    "    '''\n",
    "    threshold_save_file =  os.path.join(model_save_dir, 'threshold_dict_{}.pkl', format('-'.join([str(_) for _ in percentile_cutoff])))\n",
    "    with open(threshold_save_file, 'wb') as fh:\n",
    "        pickle.dump(threshold_save_file,fh,pickle.HIGHEST_PROTOCOL)\n",
    "    return \n",
    "\n",
    "\n",
    "'''\n",
    "Call before test mode\n",
    "'''\n",
    "\n",
    "def read_thresold_dict(percentile_cutoff:List = [2,5,10]):\n",
    "    global DIR\n",
    "    global threshold_dict\n",
    "    threshold_save_file =  os.path.join(model_save_dir, 'threshold_dict_{}.pkl', format('-'.join([str(_) for _ in percentile_cutoff])))\n",
    "    with open(threshold_save_file, 'rb') as fh:\n",
    "        threshold_dict = pickle.load(fh)\n",
    "    return \n",
    "\n",
    "def read_models():\n",
    "    global model_save_dir, model_dict\n",
    "    global MEAD_emb_list\n",
    "    global num_entities\n",
    "    global ad_model_data_dir, DIR\n",
    "    model_dict = {}\n",
    "    for emb_dim in MEAD_emb_list:\n",
    "        model_file_path = sorted(glob(os.path.join(model_save_dir, '**{}_**.pth'.format(emb_dim))))[0]\n",
    "        ad_obj = AD_model_container(\n",
    "            emb_dim,\n",
    "            num_entities\n",
    "        ) \n",
    "        ad_obj.load_model(model_file_path)\n",
    "        model_dict[emb_dim] = ad_obj\n",
    "    return \n",
    "\n",
    "'''\n",
    "This is an external facing function\n",
    "Input : a single row of pandas dataframe\n",
    "This record is not serialized is a single row of a dataframe\n",
    "'''\n",
    "def score_new_sample(record : pd.DataFrame):\n",
    "    global DIR, id_col\n",
    "    global model_dict\n",
    "    global threshold_dict\n",
    "    # perform serialization\n",
    "    serialized_record = util.convert_to_serializedID_format(\n",
    "        record, \n",
    "        DIR\n",
    "    )\n",
    "    try:\n",
    "        del serialized_record[id_col]\n",
    "    except:\n",
    "        pass\n",
    "    x_values = serialized_record.values[0]\n",
    "    x_values = x_values.reshape([1,-1])\n",
    "    result = {}\n",
    "    for emb_dim in MEAD_emb_list:\n",
    "        ad_obj = model_dict[emb_dim] \n",
    "        score = ad_obj.predict(x_values)\n",
    "        _res = {}\n",
    "        cutoff_perc_values = threshold_dict[emb]\n",
    "        for perc,v in cutoff_perc_values.items():\n",
    "            _res[perc] = (v, score[0])\n",
    "        result[emb_dim] = _res\n",
    "    return result\n",
    "\n",
    "# ======================================================================================\n",
    "if __name__ == 'main':\n",
    "    parser = argparse.ArgumentParser(description='Generate anomalies')\n",
    "    parser.add_argument('--dir', type = str, help='Which dataset ? us__import{1,2...}' ) \n",
    "    DIR = ars.dir\n",
    "    setup_config(DIR)\n",
    "    train_AD_models()\n",
    "    calculate_thresholds()\n",
    "    \n",
    "    \n",
    "# ======================================================================================\n",
    "'''\n",
    "Calling externally after initialization\n",
    "'''\n",
    "# \n",
    "# setup_config(DIR)\n",
    "# read_thresold_dict()\n",
    "# read_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171893df-1ea9-4b68-bb4e-ee3bab74963d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
