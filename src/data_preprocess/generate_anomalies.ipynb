{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83b32720-9a02-4769-9a5f-06081c279f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import multiprocessing as MP\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from collections import defaultdict,OrderedDict\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as MP\n",
    "import pickle\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob \n",
    "import yaml\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Globals\n",
    "# =======================\n",
    "\n",
    "DIR = None\n",
    "data_loc = None\n",
    "anomaly_save_dir = None\n",
    "data_df_path = None\n",
    "domain_dims = None\n",
    "test_data_path = None\n",
    "CONFIG_FILE = 'anomalyGen_config.yaml'\n",
    "id_col = 'PanjivaRecordID'\n",
    "entity_coocc_matrix = defaultdict()\n",
    "CONFIG = None\n",
    "\n",
    "def setup_config(DIR):\n",
    "    global id_col\n",
    "    global CONFIG_FILE, CONFIG\n",
    "    global data_loc, anomaly_save_dir, domain_dims, data_df_path, test_data_path\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    data_loc = CONFIG['data_dir']\n",
    "    anomaly_save_dir = os.path.join(data_loc, DIR, CONFIG['save_dir'])\n",
    "    Path(anomaly_save_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    domain_dims_file = CONFIG['domain_dims_file']\n",
    "    subDIR = DIR\n",
    "    with open(os.path.join(data_loc, subDIR, domain_dims_file), 'rb') as fh :\n",
    "        domain_dims = pickle.load(fh)\n",
    "    data_df_path = os.path.join( data_loc, DIR, CONFIG['train_data_file'])\n",
    "    test_data_path = os.path.join( data_loc, DIR, CONFIG['test_data_file'])\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "def setup_coOcc_matrix(data_df):\n",
    "    global id_col, entity_coocc_matrix, domain_dims\n",
    "    columns = list(data_df.columns)\n",
    "    columns.remove(id_col)\n",
    "    \n",
    "    def aux(pair):\n",
    "        global entity_coocc_matrix, domain_dims\n",
    "        pair = sorted(pair)\n",
    "        attr1,attr2 = pair[0], pair[1]\n",
    "        tmp_df = data_df[[attr1,attr2]]\n",
    "        key = (attr1,attr2)\n",
    "        tmp_df = tmp_df.drop_duplicates()\n",
    "        entity_coocc_matrix[key] = lil_matrix(\n",
    "            np.zeros([domain_dims[attr1],domain_dims[attr2]],dtype=bool)\n",
    "        )\n",
    "        \n",
    "        for _ , _row in tmp_df.iterrows():\n",
    "            idx1 = _row[attr1]\n",
    "            idx2 = _row[attr2]\n",
    "            entity_coocc_matrix[key][idx1,idx2] = 1\n",
    "        return \n",
    "    Parallel(n_jobs = MP.cpu_count(),backend=\"threading\")(delayed(aux)(pair,) for pair in tqdm(combinations(columns,2)))\n",
    "    return \n",
    "        \n",
    " \n",
    "\n",
    "           \n",
    "\n",
    "'''\n",
    "Anomaly type = 1  ::: \n",
    "    With repect to one of the fixed types change something\n",
    "Anomaly type = 2 :::\n",
    "    Choose any pair of attributes\n",
    "'''\n",
    "def aux_genrateAnomaly(\n",
    "    row,\n",
    "    perturb_columns: List,\n",
    "    fixed_columns: List,\n",
    "    num_perturb: int = 2,\n",
    "    anom_type:int  = 1\n",
    "):\n",
    "    global id_col, domain_dims, CONFIG\n",
    "    # Sparse matrix to speed up computation\n",
    "    global entity_coocc_matrix \n",
    "    # columns = [ _ for _ in list(df.columns) if _ not in fixed_columns + [id_col])\n",
    "   \n",
    "    new_id = int(str(row[id_col]) +  '00' + str(anom_type) + '00' + str(num_perturb))\n",
    "    new_record = row.copy()\n",
    "    new_record[id_col] = new_id\n",
    "  \n",
    "    \n",
    "    if anom_type == 1 :\n",
    "        # choose columns at random\n",
    "        attr_p = np.random.choice(perturb_columns, size=num_perturb, replace= False)\n",
    "        attr_f = np.random.choice(fixed_columns, size=num_perturb, replace= False)\n",
    "        for i in range(num_perturb):\n",
    "            domain1 = attr_p[i] # domain being perturbed\n",
    "            domain2 = attr_f[i]\n",
    "            target_domain = domain1\n",
    "            fixed_val = row[domain2]\n",
    "            _order_ = int(domain1 < domain2)\n",
    "            tmp = sorted([domain1,domain2])\n",
    "            key_d1, key_d2 = tmp[0], tmp[1]\n",
    "            key = (key_d1, key_d2)\n",
    "            \n",
    "            if _order_ == 1 : \n",
    "                choices = entity_coocc_matrix[key][:,fixed_val].toarray().reshape(-1)\n",
    "            else:\n",
    "                choices = entity_coocc_matrix[key][fixed_val,:].toarray().reshape(-1)\n",
    "            choices = np.argwhere(choices == False).reshape(-1)\n",
    "            new_record[domain1] = int(np.random.choice(choices, 1))      \n",
    "    else:\n",
    "        # Read in the matapaths\n",
    "        with open(CONFIG['anomaly_relations'], 'r') as fh:\n",
    "            lines = fh.readlines()\n",
    "            relations_list = [ _.strip().split(',') for _ in lines]\n",
    "        \n",
    "        _idx = np.random.choice(np.arange(len(relations_list)), 1)[0]\n",
    "        candidate_relation = relations_list[_idx]\n",
    "        candidiate_domains_p = np.random.choice(\n",
    "            list(set(candidate_relation).intersection(perturb_columns)), \n",
    "            num_perturb, \n",
    "            replace=False\n",
    "        )\n",
    "        candidiate_domains_f =  np.random.choice(\n",
    "            list(set(candidate_relation).intersection(fixed_columns)), \n",
    "            1, \n",
    "            replace=False\n",
    "        )\n",
    "        for i in range(num_perturb):\n",
    "            domain1 = candidiate_domains_p[i] # domain being perturbed\n",
    "            domain2 = candidiate_domains_f[0]\n",
    "            target_domain = domain1\n",
    "            fixed_val = row[domain2]\n",
    "            _order_ = int(domain1 < domain2)\n",
    "            tmp = sorted([domain1,domain2])\n",
    "            key_d1, key_d2 = tmp[0], tmp[1]\n",
    "            key = (key_d1, key_d2)\n",
    "            \n",
    "            if _order_ == 1 : \n",
    "                choices = entity_coocc_matrix[key][:,fixed_val].toarray().reshape(-1)\n",
    "            else:\n",
    "                choices = entity_coocc_matrix[key][fixed_val,:].toarray().reshape(-1)\n",
    "            choices = np.argwhere(choices == False).reshape(-1)\n",
    "            new_record[domain1] = int(np.random.choice(choices, 1))    \n",
    "            \n",
    "       \n",
    "    return new_record\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_anomalies(\n",
    "    data_df, \n",
    "    num_perturb,\n",
    "    anomaly_type  \n",
    "):\n",
    "    global id_col, anomaly_save_dir\n",
    "    fixed_columns = ['ConsigneePanjivaID', 'ShipperPanjivaID', 'HSCode']\n",
    "    perturb_columns = ['PortOfLading', 'PortOfUnlading', 'Carrier', 'ShipmentOrigin', 'ShipmentDestination']\n",
    "    results = []\n",
    "    results = Parallel(n_jobs = MP.cpu_count()) (delayed(aux_genrateAnomaly)(\n",
    "                    row,\n",
    "                    perturb_columns,\n",
    "                    fixed_columns,\n",
    "                    num_perturb,\n",
    "                    anomaly_type\n",
    "    ) for _ , row in tqdm(data_df.iterrows()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(columns=list( data_df.columns))\n",
    "    for  r in results:\n",
    "        df = df.append(r,ignore_index=True)\n",
    "    df.to_csv(os.path.join(anomaly_save_dir,'anomalies_{}_{}.csv'.format(anomaly_type, num_perturb)), index=None)\n",
    "    return\n",
    "\n",
    "# ----------------------\n",
    "parser = argparse.ArgumentParser(description='Generate anomalies')\n",
    "parser.add_argument('--dir', type = str, help='Which dataset ? us__import{1,2...}' )\n",
    "parser.add_argument('--num_perturb', type = int, default = 2, choices=[1,2,3], help='Number of entities to be perturbed. Default is 2')\n",
    "parser.add_argument('--anomaly_type', type= int, default = 1, choices=[1,2], help='Anomaly generation logic (see code comments)')\n",
    "parser.print_help()\n",
    "args = parser.parse_args()\n",
    "\n",
    "DIR = args.dir\n",
    "num_perturb = args.num_perturb\n",
    "anomaly_type = args.anomaly_type\n",
    "# ----------------------\n",
    "\n",
    "# DIR = 'us_import1'\n",
    "# num_perturb = 2\n",
    "# anomaly_type = 2\n",
    "\n",
    "setup_config(DIR)\n",
    "data_df = pd.read_csv( data_df_path, index_col=None)\n",
    "entity_coocc_matrix = defaultdict()\n",
    "setup_coOcc_matrix(data_df.copy(deep=True))\n",
    "anom_base_df = pd.read_csv( test_data_path, index_col=None)\n",
    "attribute_cols = list(anom_base_df.columns)\n",
    "attribute_cols.remove(id_col)\n",
    "anom_base_df = anom_base_df.drop_duplicates(attribute_cols)\n",
    "\n",
    "generate_anomalies(\n",
    "    anom_base_df.copy(deep=True),\n",
    "    num_perturb,\n",
    "    anomaly_type\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
