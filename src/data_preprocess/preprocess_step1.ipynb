{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4509f22b-c480-4ce5-8440-e2e154cba2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "import yaml\n",
    "import multiprocessing as MP\n",
    "from collections import OrderedDict    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pandarallel import pandarallel\n",
    "import re\n",
    "pandarallel.initialize()\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import pickle\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "\n",
    "\n",
    "DATA_SOURCE = './../../Data_Raw'\n",
    "CONFIG = None\n",
    "DIR_LOC = None\n",
    "CONFIG = None\n",
    "CONFIG_FILE = './config.yaml'\n",
    "id_col = 'PanjivaRecordID'\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "column_value_filters = None\n",
    "\n",
    "\n",
    "save_dir = None\n",
    "NUMERIC_COLUMNS = None\n",
    "DISCRETE_COLUMNS = None\n",
    "\n",
    "def set_up_config(_DIR = None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global freq_bound\n",
    "   \n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    \n",
    "    global DATA_SOURCE\n",
    "    global DIR_LOC\n",
    "    global NUMERIC_COLUMNS\n",
    "    global id_col\n",
    "    global DISCRETE_COLUMNS\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "    DATA_SOURCE = os.path.join(DATA_SOURCE, DIR_LOC)\n",
    "    save_dir =  CONFIG['save_dir']\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    save_dir = os.path.join(\n",
    "        CONFIG['save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "\n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    freq_bound = CONFIG[DIR]['low_freq_bound']\n",
    "    column_value_filters = CONFIG[DIR]['column_value_filters']\n",
    "    \n",
    "    NUMERIC_COLUMNS = CONFIG[DIR]['numeric_columns']\n",
    "    if NUMERIC_COLUMNS is None:\n",
    "        NUMERIC_COLUMNS = []  \n",
    "    \n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(id_col)\n",
    "    for nc in NUMERIC_COLUMNS:\n",
    "        try:\n",
    "            _cols.remove(nc)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    DISCRETE_COLUMNS = list(sorted(_cols))\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_regex(_type='train'):\n",
    "    global DIR\n",
    "    \n",
    "    if DIR == 'us_import1':\n",
    "        if _type == 'train':\n",
    "            return '.*0[1]_2015.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*02_2015.csv'\n",
    "\n",
    "    elif DIR == 'us_import2':\n",
    "        if _type == 'train':\n",
    "            return '.*0[3]_2016.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*04_2016.csv'\n",
    "    \n",
    "    elif DIR == 'us_import3':\n",
    "        if _type == 'train':\n",
    "            return '.*0[5]_2016.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*06_2016.csv'\n",
    "        \n",
    "\n",
    "\n",
    "def get_files(DIR, _type='all'):\n",
    "    global DATA_SOURCE\n",
    "    print(DATA_SOURCE)\n",
    "    data_dir = DATA_SOURCE\n",
    "    regex = get_regex(_type)\n",
    "    c = glob.glob(os.path.join(data_dir, '*'))\n",
    "   \n",
    "    def glob_re(pattern, strings):\n",
    "        return filter(re.compile(pattern).match, strings)\n",
    "    \n",
    "    files = sorted([_ for _ in glob_re(regex, c)])\n",
    "    print('DIR ::', DIR, ' Type ::', _type, 'Files count::', len(files))\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "def remove_low_frequency_values(df):\n",
    "    global id_col\n",
    "    global freq_bound\n",
    "    global NUMERIC_COLUMNS\n",
    "    global DISCRETE_COLUMNS\n",
    "    freq_column_value_filters = {}\n",
    "    \n",
    "    feature_cols = list(DISCRETE_COLUMNS)\n",
    "    print ('feature columns ::' , feature_cols)\n",
    "    # ----\n",
    "    # figure out which entities are to be removed\n",
    "    # ----\n",
    "    \n",
    "    counter_df = pd.DataFrame(columns=['domain', 'count'])\n",
    "    \n",
    "    for c in feature_cols:\n",
    "        count = len(set(df[c]))\n",
    "        counter_df = counter_df.append({\n",
    "            'domain': c, 'count': count\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "        z = np.percentile(\n",
    "            list(Counter(df[c]).values()), 5)\n",
    "        print(c, count, z)\n",
    "\n",
    "    counter_df = counter_df.sort_values(by=['count'], ascending=False)\n",
    "    \n",
    "    \n",
    "    for c in list(counter_df['domain']):\n",
    "        \n",
    "        values = list(df[c])\n",
    "        freq_column_value_filters[c] = []\n",
    "        obj_counter = Counter(values)\n",
    "\n",
    "        for _item, _count in obj_counter.items():\n",
    "            if _count < freq_bound:\n",
    "                freq_column_value_filters[c].append(_item)\n",
    "\n",
    "    print('Removing :: ')\n",
    "    for c, _items in freq_column_value_filters.items():\n",
    "        print('column : ', c, 'count', len(_items))\n",
    "\n",
    "    print(' DF length : ', len(df))\n",
    "    for col, val in freq_column_value_filters.items():\n",
    "        df = df.loc[~df[col].isin(val)]\n",
    "\n",
    "    print(' DF length : ', len(df))\n",
    "    return df\n",
    "\n",
    "def apply_value_filters(list_df):\n",
    "    global column_value_filters\n",
    "\n",
    "    if type(column_value_filters) != bool:\n",
    "        list_processed_df = []\n",
    "        for df in list_df:\n",
    "            for col, val in column_value_filters.items():\n",
    "                df = df.loc[~df[col].isin(val)]\n",
    "            list_processed_df.append(df)\n",
    "        return list_processed_df\n",
    "    return list_df\n",
    "\n",
    "'''\n",
    "4 digit hs code\n",
    "'''\n",
    "def HSCode_cleanup_aux(val):\n",
    "    val = val.split(';')\n",
    "    _list =['9401','9403','9201','9614','9202','9302', '9304', '6602','8201','9207','9504', '9205', '9206', '9209','9202']\n",
    "    _list =['9401', '9403','9201','9202', '9205','9206', '9207', '9209',  '9302', '9304' ]\n",
    "    val = str(val[0])\n",
    "    val = val.replace('.','')\n",
    "    val = str(val[:6])\n",
    "    \n",
    "    if val[:2] == '44': \n",
    "        return val[:4]\n",
    "    \n",
    "    elif val[:4] in _list: \n",
    "        return val \n",
    "    return val[:4]\n",
    "\n",
    "def HSCode_cleanup(list_df):\n",
    "    new_list = []\n",
    "    for _df in list_df :\n",
    "        _df['HSCode'] = _df['HSCode'].parallel_apply(HSCode_cleanup_aux)\n",
    "        _df = _df.dropna()\n",
    "        print(' In HSCode clean up , length of dataframe ', len(_df))\n",
    "        new_list.append(_df)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def clean_train_data():\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global DIR_LOC\n",
    "    \n",
    "    files = get_files(DIR, 'train')\n",
    "    print('Columns read ', use_cols)\n",
    "    list_df = [pd.read_csv(_file, usecols=use_cols, low_memory=False) for _file in files]\n",
    "    list_df = [_.dropna() for _  in list_df]\n",
    "    list_df = HSCode_cleanup(list_df)\n",
    "    list_df_1 = apply_value_filters(list_df)\n",
    "    master_df = None\n",
    "    \n",
    "    for df in list_df_1:\n",
    "        if master_df is None:\n",
    "            master_df = pd.DataFrame(df, copy=True)\n",
    "        else:\n",
    "            master_df = master_df.append(\n",
    "                df,\n",
    "                ignore_index=True\n",
    "            )\n",
    "    master_df = remove_low_frequency_values(master_df)\n",
    "    return master_df\n",
    "\n",
    "def order_cols(df):\n",
    "    global NUMERIC_COLUMNS\n",
    "    global DISCRETE_COLUMNS\n",
    "    global id_col\n",
    "    print('>>>', NUMERIC_COLUMNS)\n",
    "    ord_cols = [id_col] + DISCRETE_COLUMNS + NUMERIC_COLUMNS\n",
    "    return df[ord_cols]\n",
    "\n",
    "def convert_to_ids(\n",
    "        df,\n",
    "        save_dir\n",
    "):\n",
    "    global id_col\n",
    "    global freq_bound\n",
    "    global DISCRETE_COLUMNS\n",
    "\n",
    "    feature_columns = list(sorted(DISCRETE_COLUMNS))\n",
    "    dict_DomainDims = {}\n",
    "    col_val2id_dict = {}\n",
    "\n",
    "    for col in feature_columns:\n",
    "        vals = list(set(df[col]))\n",
    "        vals = list(sorted(vals))\n",
    "\n",
    "        id2val_dict = {\n",
    "            e[0]: e[1]\n",
    "            for e in enumerate(vals, 0)\n",
    "        }\n",
    "        print(' > ',col ,':', len(id2val_dict))\n",
    "\n",
    "        val2id_dict = {\n",
    "            v: k for k, v in id2val_dict.items()\n",
    "        }\n",
    "        col_val2id_dict[col] = val2id_dict\n",
    "\n",
    "\n",
    "        # Replace\n",
    "        df[col] = df.parallel_apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dict_DomainDims[col] = len(id2val_dict)\n",
    "\n",
    "    print(' Feature columns :: ', feature_columns)\n",
    "    print(' dict_DomainDims ', dict_DomainDims)\n",
    "    # -------------\n",
    "    # Save the domain dimensions\n",
    "    # -------------\n",
    "\n",
    "    file = 'domain_dims.pkl'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            dict_DomainDims,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "    file = 'col_val2id_dict.pkl'\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            col_val2id_dict,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "    return df, col_val2id_dict\n",
    "\n",
    "def replace_attr_with_id(row, attr, val2id_dict):\n",
    "    val = row[attr]\n",
    "    if val not in val2id_dict.keys():\n",
    "        print(attr, val)\n",
    "        return None\n",
    "    else:\n",
    "        return val2id_dict[val]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_testing_data(\n",
    "        test_df,\n",
    "        train_df,\n",
    "        col_val2id_dict\n",
    "):\n",
    "    global id_col\n",
    "    global save_dir\n",
    "    global DISCRETE_COLUMNS\n",
    "    test_df = test_df.dropna()\n",
    "\n",
    "    # Replace with None if ids are not in train_set\n",
    "    feature_cols = list(DISCRETE_COLUMNS)\n",
    "   \n",
    "    for col in feature_cols:\n",
    "        valid_items = list(col_val2id_dict[col].keys())\n",
    "        test_df = test_df.loc[test_df[col].isin(valid_items)]\n",
    "\n",
    "    # First convert to to ids\n",
    "    for col in feature_cols:\n",
    "        val2id_dict = col_val2id_dict[col]\n",
    "        test_df[col] = test_df.parallel_apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "    test_df = test_df.dropna()\n",
    "    test_df = test_df.drop_duplicates()\n",
    "    test_df = order_cols(test_df)\n",
    "\n",
    "    print(' Length of testing data', len(test_df))\n",
    "    test_df = order_cols(test_df)\n",
    "    return test_df\n",
    "\n",
    "def create_train_test_sets():\n",
    "    global use_cols\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global CONFIG\n",
    "    global DIR_LOC\n",
    "    global NUMERIC_COLUMNS\n",
    "    \n",
    "    train_df_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    test_df_file = os.path.join(save_dir, 'test_data.csv')\n",
    "    \n",
    "    train_raw_df_file = os.path.join(save_dir, 'train_data_raw.csv')\n",
    "    test_raw_df_file = os.path.join(save_dir, 'test_data_raw.csv')\n",
    "    \n",
    "    column_valuesId_dict_file = 'column_valuesId_dict.pkl'\n",
    "    column_valuesId_dict_path = os.path.join(save_dir, column_valuesId_dict_file)\n",
    "    \n",
    "    # --- Later on - remove using the saved file ---- #\n",
    "    if os.path.exists(train_df_file) and os.path.exists(test_df_file) and False:\n",
    "        train_df = pd.read_csv(train_df_file)\n",
    "        test_df = pd.read_csv(test_df_file)\n",
    "        with open(column_valuesId_dict_path, 'rb') as fh:\n",
    "            col_val2id_dict = pickle.load(fh)\n",
    "\n",
    "        return train_df, test_df, col_val2id_dict\n",
    "\n",
    "    train_df = clean_train_data()\n",
    "    train_df = order_cols(train_df)\n",
    "    train_raw_df = train_df.copy(deep=True)\n",
    "    \n",
    "    train_df, col_val2id_dict = convert_to_ids(\n",
    "        train_df,\n",
    "        save_dir\n",
    "    )\n",
    "    print('Length of train data ', len(train_df))\n",
    "    train_df = order_cols(train_df)\n",
    "\n",
    "    '''\n",
    "         test data preprocessing\n",
    "    '''\n",
    "    # combine test data into 1 file :\n",
    "    test_files = get_files(DIR, 'test')\n",
    "    list_test_df = [\n",
    "        pd.read_csv(_file, low_memory=False, usecols=use_cols)\n",
    "        for _file in test_files\n",
    "    ]\n",
    "    list_test_df = [ _.dropna() for _ in list_test_df]\n",
    "    list_test_df = HSCode_cleanup(list_test_df)\n",
    "\n",
    "    test_df = None\n",
    "    \n",
    "    for _df in list_test_df:\n",
    "        if test_df is None:\n",
    "            test_df = _df\n",
    "        else:\n",
    "            test_df = test_df.append(_df)\n",
    "\n",
    "    print('size of  Test set ', len(test_df))\n",
    "    test_raw_df = test_df.copy(deep=True)\n",
    "    test_df = setup_testing_data(\n",
    "        test_df,\n",
    "        train_df,\n",
    "        col_val2id_dict\n",
    "    )\n",
    "    train_raw_df.to_csv(train_raw_df_file, index=False)\n",
    "    test_raw_df.to_csv(test_raw_df_file, index=False)\n",
    "    \n",
    "    test_df.to_csv(test_df_file, index=False)\n",
    "    train_df.to_csv(train_df_file, index=False)\n",
    "    \n",
    "    # Save data_dimensions.csv ('column', dimension')\n",
    "    dim_df = pd.DataFrame(columns=['column','dimension'])\n",
    "    for col in DISCRETE_COLUMNS:\n",
    "        _count = len(col_val2id_dict[col])\n",
    "        dim_df = dim_df.append({'column':col, 'dimension': _count},ignore_index=True\n",
    "        )\n",
    "        \n",
    "    dim_df.to_csv(os.path.join(save_dir, 'data_dimensions.csv'), index=False)\n",
    "        \n",
    "    # -----------------------\n",
    "    # Save col_val2id_dict\n",
    "    # -----------------------\n",
    "    with open(column_valuesId_dict_path, 'wb') as fh:\n",
    "        pickle.dump(col_val2id_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return train_df, test_df, col_val2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8da74a2-7f32-4883-9d33-937eb8197078",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'us_import3'\n",
    "set_up_config(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99bc21e-071e-4954-bcca-619514a3c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../../Data_Raw/us_import\n",
      "DIR :: us_import3  Type :: train Files count:: 1\n",
      "Columns read  ['PanjivaRecordID', 'ConsigneePanjivaID', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin', 'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'HSCode']\n",
      " In HSCode clean up , length of dataframe  711455\n",
      "feature columns :: ['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n",
      "Carrier 1468 1.0\n",
      "ConsigneePanjivaID 135389 1.0\n",
      "HSCode 1155 2.0\n",
      "PortOfLading 733 1.0\n",
      "PortOfUnlading 129 1.0\n",
      "ShipmentDestination 230 1.0\n",
      "ShipmentOrigin 207 1.0\n",
      "ShipperPanjivaID 174242 1.0\n",
      "Removing :: \n",
      "column :  ShipperPanjivaID count 171522\n",
      "column :  ConsigneePanjivaID count 131866\n",
      "column :  Carrier count 623\n",
      "column :  HSCode count 317\n",
      "column :  PortOfLading count 441\n",
      "column :  ShipmentDestination count 81\n",
      "column :  ShipmentOrigin count 66\n",
      "column :  PortOfUnlading count 62\n",
      " DF length :  711455\n",
      " DF length :  146233\n",
      ">>> []\n",
      " >  Carrier : 556\n",
      " >  ConsigneePanjivaID : 2920\n",
      " >  HSCode : 777\n",
      " >  PortOfLading : 243\n",
      " >  PortOfUnlading : 62\n",
      " >  ShipmentDestination : 138\n",
      " >  ShipmentOrigin : 78\n",
      " >  ShipperPanjivaID : 2566\n",
      " Feature columns ::  ['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n",
      " dict_DomainDims  {'Carrier': 556, 'ConsigneePanjivaID': 2920, 'HSCode': 777, 'PortOfLading': 243, 'PortOfUnlading': 62, 'ShipmentDestination': 138, 'ShipmentOrigin': 78, 'ShipperPanjivaID': 2566}\n",
      "Length of train data  146233\n",
      ">>> []\n",
      "./../../Data_Raw/us_import\n",
      "DIR :: us_import3  Type :: test Files count:: 1\n",
      " In HSCode clean up , length of dataframe  682673\n",
      "size of  Test set  682673\n",
      ">>> []\n",
      " Length of testing data 127267\n",
      ">>> []\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, col_val2id_dict = create_train_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d452d0f-269e-4056-bce4-77d117138fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
