{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b378b93-5f15-4137-9788-cc03bdb483f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module\n",
    "from torch.nn import functional as F\n",
    "from torch import LongTensor as LT\n",
    "from torch import FloatTensor as FT\n",
    "from torch import nn\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device( \"cpu\")\n",
    "\n",
    "class proxy_clf_network(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        domain_dims: list = [], \n",
    "        layer_dims:list  = [32,256,256],\n",
    "        dropout_prob = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A simple 3 layered neural network, as per the paper\n",
    "        \"\"\"\n",
    "        super(proxy_clf_network, self).__init__()\n",
    "        self.domain_dims = domain_dims\n",
    "        self.num_domains = len(domain_dims)\n",
    "        self.layer_dims = layer_dims\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.__build__()\n",
    "        return\n",
    "    \n",
    "    def __build__(self):\n",
    "        \"\"\"\n",
    "        Build the architecture\n",
    "        \"\"\"\n",
    "        emb_layer_dim = self.layer_dims[0]\n",
    "        # Create an embedding layer for each domain\n",
    "        embModule_list = []\n",
    "        for  dim in self.domain_dims:\n",
    "            embModule_list.append(nn.Embedding(dim, emb_layer_dim))\n",
    "        self.embModule_list = nn.ModuleList(embModule_list)\n",
    "        \n",
    "        # The outputs should be concatenated\n",
    "        fcn_layers = []\n",
    "        dropout_prob = self.dropout_prob\n",
    "        num_layers = len(self.layer_dims)\n",
    "        inp_dim = emb_layer_dim * len(self.domain_dims)\n",
    "        for i in range(1, num_layers):\n",
    "            op_dim =  self.layer_dims[i]\n",
    "            fcn_layers.append(nn.Linear(inp_dim,op_dim))\n",
    "            fcn_layers.append(nn.Dropout(dropout_prob))\n",
    "            fcn_layers.append(nn.ReLU())\n",
    "            inp_dim = op_dim\n",
    "        \n",
    "        # Last layer for binary output\n",
    "        fcn_layers.append(nn.Linear(inp_dim, 1))\n",
    "        fcn_layers.append(nn.Sigmoid())                 \n",
    "        self.fcn = nn.Sequential(*fcn_layers)\n",
    "        return \n",
    "    \n",
    "    def forward(self,X):\n",
    "        \"\"\" \n",
    "        Input X : has shape [batch, num_domains, 1]\n",
    "        \"\"\"\n",
    "       \n",
    "        emb = []\n",
    "        for i in range(self.num_domains):\n",
    "            r = self.embModule_list[i](X[:,i])\n",
    "            emb.append(r)\n",
    "        emb = torch.cat(emb, dim =-1)\n",
    "        \n",
    "        x1 = self.fcn(emb)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class proxy_clf(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Container for the proxy model \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: proxy_clf_network,\n",
    "        dataset :str = None,\n",
    "        batch_size: int = 512,\n",
    "        LR: float = 0.001,\n",
    "        device = torch.device(\"cpu\")\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.signature = 'proxy_{}'.format(dataset) \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.LR = LR \n",
    "        return\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X : np.array, \n",
    "        Y : np.array,\n",
    "        num_epochs: int = 50,\n",
    "        log_interval = 100\n",
    "    ):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        bs = self.batch_size\n",
    "        opt = torch.optim.Adam(list(self.model.parameters()), lr = self.LR)\n",
    "        num_batches = X.shape[0] // bs + 1\n",
    "        idx = np.arange(X.shape[0])\n",
    "        loss_values = []\n",
    "        clip_value = 5\n",
    "        # train model \n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            np.random.shuffle(idx)\n",
    "            epoch_loss = []\n",
    "            for b in range(num_batches):\n",
    "                opt.zero_grad() \n",
    "                b_idx = idx[b*bs:(b+1)*bs]\n",
    "                x = LT(X[b_idx]).to(self.device) \n",
    "                pred_y = self.model(x)\n",
    "                target_y = FT(Y[b_idx]).to(self.device)\n",
    "                # Calculate loss\n",
    "                loss = F.binary_cross_entropy(pred_y, target_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_value)\n",
    "                opt.step()\n",
    "                \n",
    "                if b % log_interval == 0 :\n",
    "                    print('[Epoch] {}  | batch {} | Loss {:4f}'.format(epoch, b, loss.cpu().data.numpy()))\n",
    "                epoch_loss.append(loss.cpu().data.numpy())\n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "            loss_values.append(epoch_loss)\n",
    "        return  loss_values  \n",
    "    \n",
    "    def predict(\n",
    "        self, \n",
    "        X\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        result = []\n",
    "        with torch.no_grad():\n",
    "            bs = self.batch_size\n",
    "            num_batches = X.shape[0] // bs + 1\n",
    "            idx = np.arange(train_x_pos.shape[0])\n",
    "            for b in range(num_batches):\n",
    "                b_idx = idx[b*bs:(b+1)*bs]\n",
    "                x = LT(X[b_idx]).to(self.device)\n",
    "                pred_y = self.model(x)\n",
    "                pred_y = pred_y.cpu().data.numpy()\n",
    "                result.extend(pred_y)\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def save_model(\n",
    "        self, \n",
    "        loc: str =None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save model \n",
    "        \"\"\"\n",
    "        if loc is None:\n",
    "            loc = './saved_models'\n",
    "        path_obj = Path(loc)\n",
    "        path_obj.mkdir( parents=True, exist_ok=True )\n",
    "        loc = os.path.join(loc, self.signature  + '.pth')\n",
    "        self.save_path = loc\n",
    "        torch.save(self.model, loc)\n",
    "        return\n",
    "\n",
    "    def load_model(\n",
    "        self, \n",
    "        path: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load Model\n",
    "        \"\"\"\n",
    "        if self.save_path is None and path is None:\n",
    "            print('Error . Null path given to load model ')\n",
    "            return None\n",
    "        print('Device', self.device)\n",
    "        if path is None:\n",
    "            path = self.save_path \n",
    "        \n",
    "        self.model = torch.load(path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "401318f6-5f0e-4236-9456-092f876ee442",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../../GeneratedData/us_import1/domain_dims.pkl','rb') as fh:\n",
    "    domain_dims = OrderedDict(pickle.load(fh))\n",
    "df = pd.read_csv('./../../GeneratedData/us_import1/train_data.csv', index_col=None)\n",
    "\n",
    "try:\n",
    "    del df['PanjivaRecordID']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X = df.head(1000).values\n",
    "Y = np.random.randint(0,2, size=[1000,1])\n",
    "network = proxy_clf_network(list(domain_dims.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e90f4f3-3389-4afe-8cb3-260e9c053ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.num_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df1aa1db-4742-4909-a863-3015d8a8fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_obj = proxy_clf(\n",
    "    model = network,\n",
    "    batch_size=512,\n",
    "    device = DEVICE, \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "752a59d1-c930-415d-85b3-06298dc4f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:00<00:01, 27.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 0  | batch 0 | Loss 0.080982\n",
      "[Epoch] 1  | batch 0 | Loss 0.096085\n",
      "[Epoch] 2  | batch 0 | Loss 0.106306\n",
      "[Epoch] 3  | batch 0 | Loss 0.087081\n",
      "[Epoch] 4  | batch 0 | Loss 0.087011\n",
      "[Epoch] 5  | batch 0 | Loss 0.089132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:00<00:01, 27.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 6  | batch 0 | Loss 0.100844\n",
      "[Epoch] 7  | batch 0 | Loss 0.095563\n",
      "[Epoch] 8  | batch 0 | Loss 0.084265\n",
      "[Epoch] 9  | batch 0 | Loss 0.091712\n",
      "[Epoch] 10  | batch 0 | Loss 0.079760\n",
      "[Epoch] 11  | batch 0 | Loss 0.098931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:00<00:01, 28.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 12  | batch 0 | Loss 0.098052\n",
      "[Epoch] 13  | batch 0 | Loss 0.083551\n",
      "[Epoch] 14  | batch 0 | Loss 0.099131\n",
      "[Epoch] 15  | batch 0 | Loss 0.090963\n",
      "[Epoch] 16  | batch 0 | Loss 0.101459\n",
      "[Epoch] 17  | batch 0 | Loss 0.082238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:00<00:00, 27.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 18  | batch 0 | Loss 0.086479\n",
      "[Epoch] 19  | batch 0 | Loss 0.089221\n",
      "[Epoch] 20  | batch 0 | Loss 0.088265\n",
      "[Epoch] 21  | batch 0 | Loss 0.094852\n",
      "[Epoch] 22  | batch 0 | Loss 0.092860\n",
      "[Epoch] 23  | batch 0 | Loss 0.099797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:01<00:00, 28.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 24  | batch 0 | Loss 0.089205\n",
      "[Epoch] 25  | batch 0 | Loss 0.085608\n",
      "[Epoch] 26  | batch 0 | Loss 0.100779\n",
      "[Epoch] 27  | batch 0 | Loss 0.087697\n",
      "[Epoch] 28  | batch 0 | Loss 0.076064\n",
      "[Epoch] 29  | batch 0 | Loss 0.083351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:01<00:00, 28.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 30  | batch 0 | Loss 0.090127\n",
      "[Epoch] 31  | batch 0 | Loss 0.085253\n",
      "[Epoch] 32  | batch 0 | Loss 0.084880\n",
      "[Epoch] 33  | batch 0 | Loss 0.075837\n",
      "[Epoch] 34  | batch 0 | Loss 0.073307\n",
      "[Epoch] 35  | batch 0 | Loss 0.073080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:01<00:00, 27.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 36  | batch 0 | Loss 0.094942\n",
      "[Epoch] 37  | batch 0 | Loss 0.084815\n",
      "[Epoch] 38  | batch 0 | Loss 0.098647\n",
      "[Epoch] 39  | batch 0 | Loss 0.085531\n",
      "[Epoch] 40  | batch 0 | Loss 0.080960\n",
      "[Epoch] 41  | batch 0 | Loss 0.087804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [00:01<00:00, 28.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 42  | batch 0 | Loss 0.099845\n",
      "[Epoch] 43  | batch 0 | Loss 0.101130\n",
      "[Epoch] 44  | batch 0 | Loss 0.090926\n",
      "[Epoch] 45  | batch 0 | Loss 0.109143\n",
      "[Epoch] 46  | batch 0 | Loss 0.099153\n",
      "[Epoch] 47  | batch 0 | Loss 0.094416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 27.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch] 48  | batch 0 | Loss 0.089763\n",
      "[Epoch] 49  | batch 0 | Loss 0.079244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.118137464,\n",
       " 0.10159801,\n",
       " 0.09743577,\n",
       " 0.096901715,\n",
       " 0.10228263,\n",
       " 0.093336955,\n",
       " 0.09660366,\n",
       " 0.094229355,\n",
       " 0.09220202,\n",
       " 0.09332855,\n",
       " 0.08989188,\n",
       " 0.09619245,\n",
       " 0.09572525,\n",
       " 0.08973353,\n",
       " 0.094560504,\n",
       " 0.08983621,\n",
       " 0.093169905,\n",
       " 0.08906898,\n",
       " 0.08936441,\n",
       " 0.08913475,\n",
       " 0.09161727,\n",
       " 0.089209974,\n",
       " 0.09191942,\n",
       " 0.08674375,\n",
       " 0.09162404,\n",
       " 0.08950473,\n",
       " 0.09042431,\n",
       " 0.08846916,\n",
       " 0.088125736,\n",
       " 0.08903943,\n",
       " 0.08640144,\n",
       " 0.087355465,\n",
       " 0.08899748,\n",
       " 0.08839207,\n",
       " 0.08808977,\n",
       " 0.08728139,\n",
       " 0.08798537,\n",
       " 0.08679101,\n",
       " 0.088233605,\n",
       " 0.090156645,\n",
       " 0.08692327,\n",
       " 0.08809258,\n",
       " 0.08893137,\n",
       " 0.08816779,\n",
       " 0.086825594,\n",
       " 0.09150988,\n",
       " 0.08748473,\n",
       " 0.08837591,\n",
       " 0.08923798,\n",
       " 0.0842368]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_obj.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14d879-ed5e-46b0-b04d-d9c25ab4e935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
